{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-E_5gSaPACu",
        "outputId": "7a0a5f9e-402b-41c6-fd62-c0681c1d7c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine Learning Lab Assignment 3\n",
            "==================================================\n",
            "=== QUESTION 1: K-FOLD CROSS VALIDATION ===\n",
            "House Dataset Shape: (5000, 6)\n",
            "Columns: ['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms', 'Avg. Area Number of Bedrooms', 'Area Population', 'Price']\n",
            "Fold 1: R² = 0.9176\n",
            "Fold 2: R² = 0.9203\n",
            "Fold 3: R² = 0.9152\n",
            "Fold 4: R² = 0.9209\n",
            "Fold 5: R² = 0.9138\n",
            "\n",
            "Best R² Score from CV: 0.9209\n",
            "Final Test R² Score: 0.9147\n",
            "\n",
            "=== QUESTION 2: GRADIENT DESCENT WITH VALIDATION SET ===\n",
            "House Dataset Shape: (5000, 6)\n",
            "Columns: ['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms', 'Avg. Area Number of Bedrooms', 'Area Population', 'Price']\n",
            "Dataset split - Train: 2800, Val: 700, Test: 1500\n",
            "\n",
            "Trying learning rate: 0.001\n",
            "Learning Rate: 0.001, Validation R²: 0.6820, Test R²: 0.6490\n",
            "\n",
            "Trying learning rate: 0.01\n",
            "Learning Rate: 0.01, Validation R²: 0.9098, Test R²: 0.9148\n",
            "\n",
            "Trying learning rate: 0.1\n",
            "  Converged after 150 iterations\n",
            "Learning Rate: 0.1, Validation R²: 0.9098, Test R²: 0.9148\n",
            "\n",
            "Trying learning rate: 1\n",
            "Learning Rate: 1, Validation R²: -inf, Test R²: -inf\n",
            "\n",
            "Best Learning Rate: 0.01\n",
            "Best Validation R²: 0.9098\n",
            "\n",
            "=== QUESTION 3: CAR PRICE PREDICTION WITH PREPROCESSING ===\n",
            "Car Dataset Shape: (205, 26)\n",
            "Dropped 4 rows with missing price\n",
            "Processed Dataset Shape: (201, 30)\n",
            "Original Model R² Score: 0.8734\n",
            "PCA Components: 16 (from 29 original features)\n",
            "Explained variance ratio: 0.9552\n",
            "PCA Model R² Score: 0.8550\n",
            "PCA reduced performance by 0.0184.\n",
            "\n",
            "==================================================\n",
            "=== FINAL SUMMARY ===\n",
            "==================================================\n",
            "Q1 - Best K-Fold CV R²: 0.9209\n",
            "Q2 - Best Learning Rate: 0.01\n",
            "Q3 - Original R²: 0.8734, PCA R²: 0.8550\n",
            "Q3 - Dimensionality Reduction: 16 components\n",
            "==================================================\n",
            "Assignment completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import all necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def load_house_data():\n",
        "    \"\"\"Load USA Housing dataset with error handling\"\"\"\n",
        "    try:\n",
        "        # Try to load from local file first\n",
        "        df = pd.read_csv('USA_Housing.csv')\n",
        "    except FileNotFoundError:\n",
        "        # If local file not found, try to download from a public source\n",
        "        try:\n",
        "            # Alternative: Load from a public URL (you may need to update this URL)\n",
        "            url = 'https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv'\n",
        "            df = pd.read_csv(url)\n",
        "            # Rename columns to match expected format if needed\n",
        "            if 'median_house_value' in df.columns:\n",
        "                df = df.rename(columns={'median_house_value': 'Price'})\n",
        "        except:\n",
        "            # Create a synthetic dataset for demonstration if no real data available\n",
        "            print(\"Creating synthetic housing data for demonstration...\")\n",
        "            np.random.seed(42)\n",
        "            n_samples = 5000\n",
        "\n",
        "            # Generate synthetic features\n",
        "            avg_area_income = np.random.normal(68000, 10000, n_samples)\n",
        "            avg_area_house_age = np.random.normal(5.0, 1.0, n_samples)\n",
        "            avg_area_number_of_rooms = np.random.normal(6.5, 1.0, n_samples)\n",
        "            avg_area_number_of_bedrooms = np.random.normal(1.2, 0.3, n_samples)\n",
        "            area_population = np.random.normal(36000, 5000, n_samples)\n",
        "\n",
        "            # Generate synthetic price based on features\n",
        "            price = (avg_area_income * 0.5 +\n",
        "                    avg_area_number_of_rooms * 50000 +\n",
        "                    avg_area_number_of_bedrooms * 25000 -\n",
        "                    avg_area_house_age * 1000 +\n",
        "                    area_population * 0.1 +\n",
        "                    np.random.normal(0, 50000, n_samples))\n",
        "\n",
        "            df = pd.DataFrame({\n",
        "                'Avg. Area Income': avg_area_income,\n",
        "                'Avg. Area House Age': avg_area_house_age,\n",
        "                'Avg. Area Number of Rooms': avg_area_number_of_rooms,\n",
        "                'Avg. Area Number of Bedrooms': avg_area_number_of_bedrooms,\n",
        "                'Area Population': area_population,\n",
        "                'Price': price\n",
        "            })\n",
        "\n",
        "    print(\"House Dataset Shape:\", df.shape)\n",
        "    print(\"Columns:\", df.columns.tolist())\n",
        "    return df\n",
        "\n",
        "def k_fold_cross_validation(X, y, k=5):\n",
        "    \"\"\"Improved K-fold cross validation with proper error handling\"\"\"\n",
        "    n = len(X)\n",
        "    fold_size = n // k\n",
        "    best_beta = None\n",
        "    best_r2 = -np.inf\n",
        "    results = []\n",
        "\n",
        "    # Reset indices to ensure proper indexing\n",
        "    X = X.reset_index(drop=True)\n",
        "    y = y.reset_index(drop=True)\n",
        "\n",
        "    for i in range(k):\n",
        "        start_idx = i * fold_size\n",
        "        end_idx = (i + 1) * fold_size if i < k - 1 else n\n",
        "\n",
        "        test_indices = list(range(start_idx, end_idx))\n",
        "        train_indices = list(range(0, start_idx)) + list(range(end_idx, n))\n",
        "\n",
        "        X_train = X.iloc[train_indices]\n",
        "        X_test = X.iloc[test_indices]\n",
        "        y_train = y.iloc[train_indices]\n",
        "        y_test = y.iloc[test_indices]\n",
        "\n",
        "        # Add intercept term\n",
        "        X_train_with_intercept = np.column_stack([np.ones(len(X_train)), X_train])\n",
        "        X_test_with_intercept = np.column_stack([np.ones(len(X_test)), X_test])\n",
        "\n",
        "        try:\n",
        "            # Calculate beta using least squares: β = (X^T X)^(-1) X^T y\n",
        "            XTX = X_train_with_intercept.T @ X_train_with_intercept\n",
        "            XTX_inv = np.linalg.inv(XTX)\n",
        "            XTy = X_train_with_intercept.T @ y_train\n",
        "            beta = XTX_inv @ XTy\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = X_test_with_intercept @ beta\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "            results.append({\n",
        "                'fold': i + 1,\n",
        "                'beta': beta,\n",
        "                'r2_score': r2\n",
        "            })\n",
        "\n",
        "            print(f\"Fold {i+1}: R² = {r2:.4f}\")\n",
        "\n",
        "            if r2 > best_r2:\n",
        "                best_r2 = r2\n",
        "                best_beta = beta\n",
        "\n",
        "        except np.linalg.LinAlgError:\n",
        "            print(f\"Fold {i+1}: Singular matrix encountered, skipping...\")\n",
        "            continue\n",
        "\n",
        "    return results, best_beta, best_r2\n",
        "\n",
        "def gradient_descent(X, y, learning_rate=0.01, iterations=1000):\n",
        "    \"\"\"Improved gradient descent with convergence checking\"\"\"\n",
        "    X_with_intercept = np.column_stack([np.ones(len(X)), X])\n",
        "    m, n = X_with_intercept.shape\n",
        "    beta = np.zeros(n)\n",
        "\n",
        "    prev_cost = float('inf')\n",
        "    tolerance = 1e-6\n",
        "\n",
        "    for i in range(iterations):\n",
        "        y_pred = X_with_intercept @ beta\n",
        "        cost = np.mean((y_pred - y) ** 2)\n",
        "        gradient = (2 / m) * X_with_intercept.T @ (y_pred - y)\n",
        "        beta -= learning_rate * gradient\n",
        "\n",
        "        # Check for convergence\n",
        "        if abs(prev_cost - cost) < tolerance:\n",
        "            print(f\"  Converged after {i+1} iterations\")\n",
        "            break\n",
        "        prev_cost = cost\n",
        "\n",
        "    return beta\n",
        "\n",
        "def question_1():\n",
        "    \"\"\"Question 1: K-Fold Cross Validation for Multiple Linear Regression\"\"\"\n",
        "    print(\"=== QUESTION 1: K-FOLD CROSS VALIDATION ===\")\n",
        "    df = load_house_data()\n",
        "\n",
        "    # Handle different possible column names\n",
        "    price_col = 'Price' if 'Price' in df.columns else df.columns[-1]\n",
        "    X = df.drop(price_col, axis=1)\n",
        "    y = df[price_col]\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "    # Perform K-fold cross validation\n",
        "    results, best_beta, best_r2 = k_fold_cross_validation(X_scaled, y)\n",
        "    print(f\"\\nBest R² Score from CV: {best_r2:.4f}\")\n",
        "\n",
        "    # Final evaluation on 70-30 split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    X_train_with_intercept = np.column_stack([np.ones(len(X_train)), X_train])\n",
        "    X_test_with_intercept = np.column_stack([np.ones(len(X_test)), X_test])\n",
        "\n",
        "    if best_beta is not None:\n",
        "        y_pred_test = X_test_with_intercept @ best_beta\n",
        "        final_r2 = r2_score(y_test, y_pred_test)\n",
        "        print(f\"Final Test R² Score: {final_r2:.4f}\")\n",
        "    else:\n",
        "        print(\"No valid beta found from cross-validation\")\n",
        "        return None, None\n",
        "\n",
        "    return results, best_beta\n",
        "\n",
        "def question_2():\n",
        "    \"\"\"Question 2: Validation Set Concept with Gradient Descent\"\"\"\n",
        "    print(\"\\n=== QUESTION 2: GRADIENT DESCENT WITH VALIDATION SET ===\")\n",
        "    df = load_house_data()\n",
        "\n",
        "    # Handle different possible column names\n",
        "    price_col = 'Price' if 'Price' in df.columns else df.columns[-1]\n",
        "    X = df.drop(price_col, axis=1)\n",
        "    y = df[price_col]\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Split data: train (56%), validation (14%), test (30%)\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(f\"Dataset split - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "\n",
        "    learning_rates = [0.001, 0.01, 0.1, 1]\n",
        "    best_lr = None\n",
        "    best_val_r2 = -np.inf\n",
        "    best_coefficients = None\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        print(f\"\\nTrying learning rate: {lr}\")\n",
        "        try:\n",
        "            coefficients = gradient_descent(X_train, y_train, lr, 1000)\n",
        "\n",
        "            # Make predictions\n",
        "            X_val_with_intercept = np.column_stack([np.ones(len(X_val)), X_val])\n",
        "            X_test_with_intercept = np.column_stack([np.ones(len(X_test)), X_test])\n",
        "\n",
        "            y_val_pred = X_val_with_intercept @ coefficients\n",
        "            y_test_pred = X_test_with_intercept @ coefficients\n",
        "\n",
        "            val_r2 = r2_score(y_val, y_val_pred)\n",
        "            test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "            print(f\"Learning Rate: {lr}, Validation R²: {val_r2:.4f}, Test R²: {test_r2:.4f}\")\n",
        "\n",
        "            if val_r2 > best_val_r2:\n",
        "                best_val_r2 = val_r2\n",
        "                best_lr = lr\n",
        "                best_coefficients = coefficients\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with learning rate {lr}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\nBest Learning Rate: {best_lr}\")\n",
        "    print(f\"Best Validation R²: {best_val_r2:.4f}\")\n",
        "\n",
        "    return best_coefficients, best_lr\n",
        "\n",
        "def load_car_data():\n",
        "    \"\"\"Load car dataset with proper error handling\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\"\n",
        "    columns = [\"symboling\", \"normalized_losses\", \"make\", \"fuel_type\", \"aspiration\",\n",
        "               \"num_doors\", \"body_style\", \"drive_wheels\", \"engine_location\",\n",
        "               \"wheel_base\", \"length\", \"width\", \"height\", \"curb_weight\",\n",
        "               \"engine_type\", \"num_cylinders\", \"engine_size\", \"fuel_system\",\n",
        "               \"bore\", \"stroke\", \"compression_ratio\", \"horsepower\", \"peak_rpm\",\n",
        "               \"city_mpg\", \"highway_mpg\", \"price\"]\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(url, names=columns, na_values='?')\n",
        "        print(\"Car Dataset Shape:\", df.shape)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading car dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_car_data(df):\n",
        "    \"\"\"Improved preprocessing with better error handling\"\"\"\n",
        "    if df is None:\n",
        "        return None\n",
        "\n",
        "    # Make a copy to avoid modifying original\n",
        "    df = df.copy()\n",
        "\n",
        "    # Drop rows with missing price (target variable)\n",
        "    initial_rows = len(df)\n",
        "    df = df.dropna(subset=['price'])\n",
        "    print(f\"Dropped {initial_rows - len(df)} rows with missing price\")\n",
        "\n",
        "    # Handle missing values\n",
        "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Fill numeric columns with median\n",
        "    for col in numeric_columns:\n",
        "        if col != 'price':\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    # Fill categorical columns with mode\n",
        "    for col in categorical_columns:\n",
        "        mode_val = df[col].mode()\n",
        "        if len(mode_val) > 0:\n",
        "            df[col] = df[col].fillna(mode_val[0])\n",
        "\n",
        "    # Convert word numbers to figures\n",
        "    door_mapping = {'two': 2, 'four': 4}\n",
        "    cylinder_mapping = {'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'eight': 8, 'twelve': 12}\n",
        "\n",
        "    if 'num_doors' in df.columns:\n",
        "        df['num_doors'] = df['num_doors'].map(door_mapping).fillna(df['num_doors'])\n",
        "    if 'num_cylinders' in df.columns:\n",
        "        df['num_cylinders'] = df['num_cylinders'].map(cylinder_mapping).fillna(df['num_cylinders'])\n",
        "\n",
        "    # Dummy encoding for categorical variables\n",
        "    categorical_for_dummies = ['body_style', 'drive_wheels']\n",
        "    for col in categorical_for_dummies:\n",
        "        if col in df.columns:\n",
        "            dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
        "            df = pd.concat([df, dummies], axis=1)\n",
        "            df = df.drop(col, axis=1)\n",
        "\n",
        "    # Label encoding for specific columns\n",
        "    label_encode_cols = ['make', 'aspiration', 'engine_location', 'fuel_type']\n",
        "    for col in label_encode_cols:\n",
        "        if col in df.columns:\n",
        "            le = LabelEncoder()\n",
        "            df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "    # Binary encoding for fuel_system and engine_type\n",
        "    if 'fuel_system' in df.columns:\n",
        "        df['fuel_system'] = df['fuel_system'].apply(lambda x: 1 if 'pfi' in str(x).lower() else 0)\n",
        "    if 'engine_type' in df.columns:\n",
        "        df['engine_type'] = df['engine_type'].apply(lambda x: 1 if 'ohc' in str(x).lower() else 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "def question_3():\n",
        "    \"\"\"Question 3: Car Price Prediction with Preprocessing and PCA\"\"\"\n",
        "    print(\"\\n=== QUESTION 3: CAR PRICE PREDICTION WITH PREPROCESSING ===\")\n",
        "    df = load_car_data()\n",
        "\n",
        "    if df is None:\n",
        "        print(\"Failed to load car dataset\")\n",
        "        return None, None, None\n",
        "\n",
        "    df_processed = preprocess_car_data(df)\n",
        "\n",
        "    if df_processed is None:\n",
        "        print(\"Failed to preprocess car dataset\")\n",
        "        return None, None, None\n",
        "\n",
        "    print(f\"Processed Dataset Shape: {df_processed.shape}\")\n",
        "\n",
        "    # Prepare features and target\n",
        "    X = df_processed.drop('price', axis=1)\n",
        "    y = df_processed['price']\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Original model without PCA\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    r2_original = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Original Model R² Score: {r2_original:.4f}\")\n",
        "\n",
        "    # Model with PCA\n",
        "    pca = PCA(n_components=0.95)  # Preserve 95% of variance\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "\n",
        "    print(f\"PCA Components: {pca.n_components_} (from {X.shape[1]} original features)\")\n",
        "    print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
        "\n",
        "    model_pca = LinearRegression()\n",
        "    model_pca.fit(X_train_pca, y_train)\n",
        "    y_pred_pca = model_pca.predict(X_test_pca)\n",
        "    r2_pca = r2_score(y_test, y_pred_pca)\n",
        "\n",
        "    print(f\"PCA Model R² Score: {r2_pca:.4f}\")\n",
        "\n",
        "    # Performance comparison\n",
        "    improvement = r2_pca - r2_original\n",
        "    if improvement > 0.01:\n",
        "        print(f\"PCA improved performance by {improvement:.4f}!\")\n",
        "    elif improvement < -0.01:\n",
        "        print(f\"PCA reduced performance by {-improvement:.4f}.\")\n",
        "    else:\n",
        "        print(\"PCA had no significant impact on performance.\")\n",
        "\n",
        "    return r2_original, r2_pca, pca.n_components_\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run all questions\"\"\"\n",
        "    print(\"Machine Learning Lab Assignment 3\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        # Question 1\n",
        "        results_q1, best_beta = question_1()\n",
        "\n",
        "        # Question 2\n",
        "        best_coeffs, best_lr = question_2()\n",
        "\n",
        "        # Question 3\n",
        "        r2_orig, r2_pca, n_components = question_3()\n",
        "\n",
        "        # Summary\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"=== FINAL SUMMARY ===\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        if results_q1 is not None:\n",
        "            best_cv_r2 = max([r['r2_score'] for r in results_q1])\n",
        "            print(f\"Q1 - Best K-Fold CV R²: {best_cv_r2:.4f}\")\n",
        "        else:\n",
        "            print(\"Q1 - K-Fold CV: Failed\")\n",
        "\n",
        "        if best_lr is not None:\n",
        "            print(f\"Q2 - Best Learning Rate: {best_lr}\")\n",
        "        else:\n",
        "            print(\"Q2 - Gradient Descent: Failed\")\n",
        "\n",
        "        if r2_orig is not None and r2_pca is not None:\n",
        "            print(f\"Q3 - Original R²: {r2_orig:.4f}, PCA R²: {r2_pca:.4f}\")\n",
        "            print(f\"Q3 - Dimensionality Reduction: {n_components} components\")\n",
        "        else:\n",
        "            print(\"Q3 - Car Price Prediction: Failed\")\n",
        "\n",
        "        print(\"=\" * 50)\n",
        "        print(\"Assignment completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}