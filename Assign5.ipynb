{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTl99lX_7IaT",
        "outputId": "594bf4b3-fb0f-420e-fecf-0e9e49924710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Dimensions: (1000, 8)\n",
            "\n",
            "Feature Correlation Matrix:\n",
            "       X1     X2     X3     X4     X5     X6     X7\n",
            "X1  1.000  0.914  0.906  0.912  0.915  0.910  0.917\n",
            "X2  0.914  1.000  0.915  0.919  0.916  0.911  0.910\n",
            "X3  0.906  0.915  1.000  0.913  0.914  0.910  0.909\n",
            "X4  0.912  0.919  0.913  1.000  0.910  0.912  0.915\n",
            "X5  0.915  0.916  0.914  0.910  1.000  0.912  0.916\n",
            "X6  0.910  0.911  0.910  0.912  0.912  1.000  0.909\n",
            "X7  0.917  0.910  0.909  0.915  0.916  0.909  1.000\n",
            "\n",
            "================================================================================\n",
            "CONDUCTING GRID SEARCH OVER HYPERPARAMETERS\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:59: RuntimeWarning: overflow encountered in square\n",
            "  mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
            "/tmp/ipython-input-1049011234.py:90: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if step > 0 and abs(self.cost_history[-2] - current_cost) < self.tolerance:\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: overflow encountered in square\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:77: RuntimeWarning: overflow encountered in matmul\n",
            "  weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
            "/tmp/ipython-input-1049011234.py:56: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ weights + bias\n",
            "/tmp/ipython-input-1049011234.py:73: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:59: RuntimeWarning: overflow encountered in square\n",
            "  mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
            "/tmp/ipython-input-1049011234.py:90: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if step > 0 and abs(self.cost_history[-2] - current_cost) < self.tolerance:\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: overflow encountered in square\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:77: RuntimeWarning: overflow encountered in matmul\n",
            "  weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
            "/tmp/ipython-input-1049011234.py:56: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ weights + bias\n",
            "/tmp/ipython-input-1049011234.py:73: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:59: RuntimeWarning: overflow encountered in square\n",
            "  mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
            "/tmp/ipython-input-1049011234.py:90: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if step > 0 and abs(self.cost_history[-2] - current_cost) < self.tolerance:\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: overflow encountered in square\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:77: RuntimeWarning: overflow encountered in matmul\n",
            "  weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
            "/tmp/ipython-input-1049011234.py:56: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ weights + bias\n",
            "/tmp/ipython-input-1049011234.py:73: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:59: RuntimeWarning: overflow encountered in square\n",
            "  mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
            "/tmp/ipython-input-1049011234.py:90: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if step > 0 and abs(self.cost_history[-2] - current_cost) < self.tolerance:\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: overflow encountered in square\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:77: RuntimeWarning: overflow encountered in matmul\n",
            "  weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
            "/tmp/ipython-input-1049011234.py:56: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ weights + bias\n",
            "/tmp/ipython-input-1049011234.py:73: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:59: RuntimeWarning: overflow encountered in square\n",
            "  mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
            "/tmp/ipython-input-1049011234.py:90: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if step > 0 and abs(self.cost_history[-2] - current_cost) < self.tolerance:\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: invalid value encountered in scalar multiply\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: overflow encountered in square\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:77: RuntimeWarning: overflow encountered in matmul\n",
            "  weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
            "/tmp/ipython-input-1049011234.py:56: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ weights + bias\n",
            "/tmp/ipython-input-1049011234.py:73: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ self.weights + self.bias\n",
            "/tmp/ipython-input-1049011234.py:78: RuntimeWarning: invalid value encountered in multiply\n",
            "  (self.lambda_reg / n) * self.weights\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:59: RuntimeWarning: overflow encountered in square\n",
            "  mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
            "/tmp/ipython-input-1049011234.py:90: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if step > 0 and abs(self.cost_history[-2] - current_cost) < self.tolerance:\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: overflow encountered in square\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:77: RuntimeWarning: overflow encountered in matmul\n",
            "  weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
            "/tmp/ipython-input-1049011234.py:56: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ weights + bias\n",
            "/tmp/ipython-input-1049011234.py:73: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:59: RuntimeWarning: overflow encountered in square\n",
            "  mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
            "/tmp/ipython-input-1049011234.py:90: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if step > 0 and abs(self.cost_history[-2] - current_cost) < self.tolerance:\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: overflow encountered in square\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:77: RuntimeWarning: overflow encountered in matmul\n",
            "  weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
            "/tmp/ipython-input-1049011234.py:56: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ weights + bias\n",
            "/tmp/ipython-input-1049011234.py:73: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:90: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if step > 0 and abs(self.cost_history[-2] - current_cost) < self.tolerance:\n",
            "/tmp/ipython-input-1049011234.py:59: RuntimeWarning: overflow encountered in square\n",
            "  mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: overflow encountered in square\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:77: RuntimeWarning: overflow encountered in matmul\n",
            "  weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
            "/tmp/ipython-input-1049011234.py:56: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ weights + bias\n",
            "/tmp/ipython-input-1049011234.py:73: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:59: RuntimeWarning: overflow encountered in square\n",
            "  mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: overflow encountered in square\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:90: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if step > 0 and abs(self.cost_history[-2] - current_cost) < self.tolerance:\n",
            "/tmp/ipython-input-1049011234.py:77: RuntimeWarning: overflow encountered in matmul\n",
            "  weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:56: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ weights + bias\n",
            "/tmp/ipython-input-1049011234.py:73: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ self.weights + self.bias\n",
            "/tmp/ipython-input-1049011234.py:59: RuntimeWarning: overflow encountered in square\n",
            "  mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: overflow encountered in square\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:90: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if step > 0 and abs(self.cost_history[-2] - current_cost) < self.tolerance:\n",
            "/tmp/ipython-input-1049011234.py:77: RuntimeWarning: overflow encountered in matmul\n",
            "  weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:56: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ weights + bias\n",
            "/tmp/ipython-input-1049011234.py:73: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ self.weights + self.bias\n",
            "/tmp/ipython-input-1049011234.py:59: RuntimeWarning: overflow encountered in square\n",
            "  mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: overflow encountered in square\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:90: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if step > 0 and abs(self.cost_history[-2] - current_cost) < self.tolerance:\n",
            "/tmp/ipython-input-1049011234.py:77: RuntimeWarning: overflow encountered in matmul\n",
            "  weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:56: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ weights + bias\n",
            "/tmp/ipython-input-1049011234.py:73: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ self.weights + self.bias\n",
            "/tmp/ipython-input-1049011234.py:59: RuntimeWarning: overflow encountered in square\n",
            "  mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: overflow encountered in square\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:90: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if step > 0 and abs(self.cost_history[-2] - current_cost) < self.tolerance:\n",
            "/tmp/ipython-input-1049011234.py:77: RuntimeWarning: overflow encountered in matmul\n",
            "  weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:56: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ weights + bias\n",
            "/tmp/ipython-input-1049011234.py:73: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ self.weights + self.bias\n",
            "/tmp/ipython-input-1049011234.py:59: RuntimeWarning: overflow encountered in square\n",
            "  mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: overflow encountered in square\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: invalid value encountered in scalar multiply\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:77: RuntimeWarning: overflow encountered in matmul\n",
            "  weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:56: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ weights + bias\n",
            "/tmp/ipython-input-1049011234.py:73: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ self.weights + self.bias\n",
            "/tmp/ipython-input-1049011234.py:78: RuntimeWarning: invalid value encountered in multiply\n",
            "  (self.lambda_reg / n) * self.weights\n",
            "/tmp/ipython-input-1049011234.py:59: RuntimeWarning: overflow encountered in square\n",
            "  mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: overflow encountered in square\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:90: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if step > 0 and abs(self.cost_history[-2] - current_cost) < self.tolerance:\n",
            "/tmp/ipython-input-1049011234.py:77: RuntimeWarning: overflow encountered in matmul\n",
            "  weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:56: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ weights + bias\n",
            "/tmp/ipython-input-1049011234.py:73: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ self.weights + self.bias\n",
            "/tmp/ipython-input-1049011234.py:59: RuntimeWarning: overflow encountered in square\n",
            "  mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: overflow encountered in square\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:90: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if step > 0 and abs(self.cost_history[-2] - current_cost) < self.tolerance:\n",
            "/tmp/ipython-input-1049011234.py:77: RuntimeWarning: overflow encountered in matmul\n",
            "  weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
            "/tmp/ipython-input-1049011234.py:56: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ weights + bias\n",
            "/tmp/ipython-input-1049011234.py:73: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:59: RuntimeWarning: overflow encountered in square\n",
            "  mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-1049011234.py:60: RuntimeWarning: overflow encountered in square\n",
            "  reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
            "/tmp/ipython-input-1049011234.py:90: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  if step > 0 and abs(self.cost_history[-2] - current_cost) < self.tolerance:\n",
            "/tmp/ipython-input-1049011234.py:77: RuntimeWarning: overflow encountered in matmul\n",
            "  weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
            "/tmp/ipython-input-1049011234.py:56: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ weights + bias\n",
            "/tmp/ipython-input-1049011234.py:73: RuntimeWarning: invalid value encountered in matmul\n",
            "  predictions = X @ self.weights + self.bias\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "OPTIMAL PARAMETERS (Minimum Cost)\n",
            "================================================================================\n",
            "Learning Rate: 0.1\n",
            "Regularization Strength: 1e-15\n",
            "Training Cost: 0.126831\n",
            "Test MSE: 0.258424\n",
            "Train RÂ²: 0.938961\n",
            "Test RÂ²: 0.934680\n",
            "Steps: 737\n",
            "\n",
            "================================================================================\n",
            "OPTIMAL PARAMETERS (Maximum Test RÂ²)\n",
            "================================================================================\n",
            "Learning Rate: 0.1\n",
            "Regularization Strength: 1e-15\n",
            "Training Cost: 0.126831\n",
            "Test MSE: 0.258424\n",
            "Train RÂ²: 0.938961\n",
            "Test RÂ²: 0.934680\n",
            "Steps: 737\n",
            "\n",
            "================================================================================\n",
            "FINAL MODEL EVALUATION\n",
            "================================================================================\n",
            "Train RÂ² Score: 0.938961\n",
            "Test RÂ² Score: 0.934680\n",
            "Train MSE: 0.253661\n",
            "Test MSE: 0.258424\n",
            "\n",
            "================================================================================\n",
            "TOP 10 HYPERPARAMETER COMBINATIONS (by Test RÂ²)\n",
            "================================================================================\n",
            " learning_rate       lambda  train_cost  test_mse  train_r2  test_r2\n",
            "          0.10 1.000000e-15    0.126831  0.258424  0.938961 0.934680\n",
            "          0.10 0.000000e+00    0.126831  0.258424  0.938961 0.934680\n",
            "          0.10 1.000000e-10    0.126831  0.258424  0.938961 0.934680\n",
            "          0.10 1.000000e-05    0.126831  0.258424  0.938961 0.934680\n",
            "          0.10 1.000000e-03    0.126832  0.258424  0.938961 0.934680\n",
            "          0.10 1.000000e+00    0.128427  0.258508  0.938949 0.934659\n",
            "          0.10 1.000000e+01    0.141303  0.261641  0.938235 0.933867\n",
            "          0.10 2.000000e+01    0.153203  0.267856  0.936738 0.932296\n",
            "          0.01 1.000000e-15    0.142816  0.290735  0.931268 0.926513\n",
            "          0.01 0.000000e+00    0.142816  0.290735  0.931268 0.926513\n",
            "\n",
            "âœ“ Results exported to 'ridge_regression_results.csv'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# ============================================\n",
        "# PART 1: Create Synthetic Dataset\n",
        "# ============================================\n",
        "\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Build base feature with controlled correlation\n",
        "base_feature = np.random.randn(n_samples, 1)\n",
        "features = np.zeros((n_samples, 7))\n",
        "\n",
        "# Generate 7 interdependent features with minimal noise\n",
        "for col_idx in range(7):\n",
        "    small_noise = np.random.randn(n_samples, 1) * 0.3\n",
        "    features[:, col_idx] = (base_feature + small_noise).flatten()\n",
        "\n",
        "# Create regression target from features\n",
        "coefficients = np.random.randn(7)\n",
        "target = features @ coefficients + np.random.randn(n_samples) * 0.5\n",
        "\n",
        "# Organize into DataFrame\n",
        "col_names = [f'X{j+1}' for j in range(7)]\n",
        "data = pd.DataFrame(features, columns=col_names)\n",
        "data['target'] = target\n",
        "\n",
        "print(\"Data Dimensions:\", data.shape)\n",
        "print(\"\\nFeature Correlation Matrix:\")\n",
        "print(data[col_names].corr().round(3))\n",
        "\n",
        "# ============================================\n",
        "# PART 2: Custom Ridge Regression Class\n",
        "# ============================================\n",
        "\n",
        "class RidgeRegressionOptimizer:\n",
        "    \"\"\"Custom Ridge Regression using Stochastic Gradient Descent\"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=0.01, lambda_reg=1.0,\n",
        "                 n_iterations=1000, tolerance=1e-6):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.n_iterations = n_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.cost_history = []\n",
        "\n",
        "    def _ridge_cost(self, X, y, weights, bias):\n",
        "        \"\"\"Calculate cost: MSE + regularization penalty\"\"\"\n",
        "        n = len(y)\n",
        "        predictions = X @ weights + bias\n",
        "        error = predictions - y\n",
        "\n",
        "        mse_loss = (1 / (2 * n)) * np.sum(error ** 2)\n",
        "        reg_penalty = (self.lambda_reg / (2 * n)) * np.sum(weights ** 2)\n",
        "\n",
        "        return mse_loss + reg_penalty\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train model using gradient descent\"\"\"\n",
        "        n, d = X.shape\n",
        "\n",
        "        self.weights = np.zeros(d)\n",
        "        self.bias = 0\n",
        "\n",
        "        for step in range(self.n_iterations):\n",
        "            # Compute predictions and errors\n",
        "            predictions = X @ self.weights + self.bias\n",
        "            residuals = predictions - y\n",
        "\n",
        "            # Calculate gradients\n",
        "            weight_gradient = (1 / n) * (X.T @ residuals) + \\\n",
        "                            (self.lambda_reg / n) * self.weights\n",
        "            bias_gradient = (1 / n) * np.sum(residuals)\n",
        "\n",
        "            # Update parameters\n",
        "            self.weights -= self.learning_rate * weight_gradient\n",
        "            self.bias -= self.learning_rate * bias_gradient\n",
        "\n",
        "            # Track cost\n",
        "            current_cost = self._ridge_cost(X, y, self.weights, self.bias)\n",
        "            self.cost_history.append(current_cost)\n",
        "\n",
        "            # Check for convergence\n",
        "            if step > 0 and abs(self.cost_history[-2] - current_cost) < self.tolerance:\n",
        "                break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Generate predictions\"\"\"\n",
        "        return X @ self.weights + self.bias\n",
        "\n",
        "# ============================================\n",
        "# PART 3: Prepare and Split Data\n",
        "# ============================================\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features, target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features for stable gradient descent\n",
        "normalizer = StandardScaler()\n",
        "X_train_norm = normalizer.fit_transform(X_train)\n",
        "X_test_norm = normalizer.transform(X_test)\n",
        "\n",
        "# ============================================\n",
        "# PART 4: Systematic Hyperparameter Search\n",
        "# ============================================\n",
        "\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "lambda_values = [1e-15, 1e-10, 1e-5, 1e-3, 0, 1, 10, 20]\n",
        "\n",
        "experiment_results = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONDUCTING GRID SEARCH OVER HYPERPARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for lam in lambda_values:\n",
        "        try:\n",
        "            # Initialize and train\n",
        "            regressor = RidgeRegressionOptimizer(\n",
        "                learning_rate=lr,\n",
        "                lambda_reg=lam,\n",
        "                n_iterations=1000,\n",
        "                tolerance=1e-8\n",
        "            )\n",
        "            regressor.fit(X_train_norm, y_train)\n",
        "\n",
        "            # Generate predictions\n",
        "            train_pred = regressor.predict(X_train_norm)\n",
        "            test_pred = regressor.predict(X_test_norm)\n",
        "\n",
        "            # Calculate metrics\n",
        "            final_cost = regressor.cost_history[-1]\n",
        "            test_error = mean_squared_error(y_test, test_pred)\n",
        "            train_score = r2_score(y_train, train_pred)\n",
        "            test_score = r2_score(y_test, test_pred)\n",
        "\n",
        "            experiment_results.append({\n",
        "                'learning_rate': lr,\n",
        "                'lambda': lam,\n",
        "                'train_cost': final_cost,\n",
        "                'test_mse': test_error,\n",
        "                'train_r2': train_score,\n",
        "                'test_r2': test_score,\n",
        "                'iterations': len(regressor.cost_history)\n",
        "            })\n",
        "\n",
        "        except:\n",
        "            # Handle failed runs\n",
        "            experiment_results.append({\n",
        "                'learning_rate': lr,\n",
        "                'lambda': lam,\n",
        "                'train_cost': np.inf,\n",
        "                'test_mse': np.inf,\n",
        "                'train_r2': -np.inf,\n",
        "                'test_r2': -np.inf,\n",
        "                'iterations': 0\n",
        "            })\n",
        "\n",
        "results_table = pd.DataFrame(experiment_results)\n",
        "\n",
        "# ============================================\n",
        "# PART 5: Identify Optimal Parameters\n",
        "# ============================================\n",
        "\n",
        "valid_results = results_table[results_table['train_cost'] != np.inf]\n",
        "\n",
        "# Best by minimum cost\n",
        "min_cost_idx = valid_results['train_cost'].idxmin()\n",
        "best_by_cost = valid_results.loc[min_cost_idx]\n",
        "\n",
        "# Best by maximum RÂ² on test data\n",
        "max_r2_idx = valid_results['test_r2'].idxmax()\n",
        "best_by_r2 = valid_results.loc[max_r2_idx]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OPTIMAL PARAMETERS (Minimum Cost)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Learning Rate: {best_by_cost['learning_rate']}\")\n",
        "print(f\"Regularization Strength: {best_by_cost['lambda']}\")\n",
        "print(f\"Training Cost: {best_by_cost['train_cost']:.6f}\")\n",
        "print(f\"Test MSE: {best_by_cost['test_mse']:.6f}\")\n",
        "print(f\"Train RÂ²: {best_by_cost['train_r2']:.6f}\")\n",
        "print(f\"Test RÂ²: {best_by_cost['test_r2']:.6f}\")\n",
        "print(f\"Steps: {int(best_by_cost['iterations'])}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OPTIMAL PARAMETERS (Maximum Test RÂ²)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Learning Rate: {best_by_r2['learning_rate']}\")\n",
        "print(f\"Regularization Strength: {best_by_r2['lambda']}\")\n",
        "print(f\"Training Cost: {best_by_r2['train_cost']:.6f}\")\n",
        "print(f\"Test MSE: {best_by_r2['test_mse']:.6f}\")\n",
        "print(f\"Train RÂ²: {best_by_r2['train_r2']:.6f}\")\n",
        "print(f\"Test RÂ²: {best_by_r2['test_r2']:.6f}\")\n",
        "print(f\"Steps: {int(best_by_r2['iterations'])}\")\n",
        "\n",
        "# ============================================\n",
        "# PART 6: Train Final Model\n",
        "# ============================================\n",
        "\n",
        "optimal_lr = best_by_r2['learning_rate']\n",
        "optimal_lambda = best_by_r2['lambda']\n",
        "\n",
        "final_regressor = RidgeRegressionOptimizer(\n",
        "    learning_rate=optimal_lr,\n",
        "    lambda_reg=optimal_lambda,\n",
        "    n_iterations=1000,\n",
        "    tolerance=1e-8\n",
        ")\n",
        "final_regressor.fit(X_train_norm, y_train)\n",
        "\n",
        "train_final_pred = final_regressor.predict(X_train_norm)\n",
        "test_final_pred = final_regressor.predict(X_test_norm)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL MODEL EVALUATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Train RÂ² Score: {r2_score(y_train, train_final_pred):.6f}\")\n",
        "print(f\"Test RÂ² Score: {r2_score(y_test, test_final_pred):.6f}\")\n",
        "print(f\"Train MSE: {mean_squared_error(y_train, train_final_pred):.6f}\")\n",
        "print(f\"Test MSE: {mean_squared_error(y_test, test_final_pred):.6f}\")\n",
        "\n",
        "# ============================================\n",
        "# PART 7: Summary of Top Configurations\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOP 10 HYPERPARAMETER COMBINATIONS (by Test RÂ²)\")\n",
        "print(\"=\"*80)\n",
        "top_configs = valid_results.nlargest(\n",
        "    10, 'test_r2'\n",
        ")[['learning_rate', 'lambda', 'train_cost', 'test_mse', 'train_r2', 'test_r2']]\n",
        "print(top_configs.to_string(index=False))\n",
        "\n",
        "# Save results\n",
        "results_table.to_csv('ridge_regression_results.csv', index=False)\n",
        "print(\"\\nâœ“ Results exported to 'ridge_regression_results.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# STEP 1: Load & Clean Dataset\n",
        "# ============================================\n",
        "\n",
        "dataset = pd.read_csv('/content/Hitters.csv')\n",
        "print(\"=\"*70)\n",
        "print(\"BASEBALL SALARY PREDICTION: LINEAR vs RIDGE vs LASSO\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n[STEP 1] DATASET ACQUISITION & CLEANING\")\n",
        "print(f\"  Initial Records: {len(dataset):,}\")\n",
        "print(f\"  Total Null Values: {dataset.isnull().sum().sum()}\")\n",
        "\n",
        "# Remove missing records\n",
        "dataset = dataset.dropna()\n",
        "print(f\"  After Null Removal: {len(dataset):,}\")\n",
        "\n",
        "# Eliminate duplicates\n",
        "dataset = dataset.drop_duplicates()\n",
        "print(f\"  After Deduplication: {len(dataset):,}\")\n",
        "\n",
        "# Transform categorical variables to numeric\n",
        "string_features = [col for col in dataset.columns if dataset[col].dtype == 'object']\n",
        "for feature in string_features:\n",
        "    encoder = LabelEncoder()\n",
        "    dataset[feature] = encoder.fit_transform(dataset[feature])\n",
        "\n",
        "print(f\"  Categorical Features Encoded: {len(string_features)}\")\n",
        "print(f\"  âœ“ Data cleaning complete\\n\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# STEP 2: Split & Normalize\n",
        "# ============================================\n",
        "\n",
        "print(\"[STEP 2] FEATURE ENGINEERING & NORMALIZATION\")\n",
        "\n",
        "output = 'Salary'\n",
        "inputs = dataset.drop(columns=[output])\n",
        "target = dataset[output]\n",
        "\n",
        "print(f\"  Input Features (X): {inputs.shape[0]:,} samples Ã— {inputs.shape[1]} variables\")\n",
        "print(f\"  Output Variable (y): {target.shape[0]:,} samples\")\n",
        "\n",
        "# Partition dataset\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(inputs, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "norm = StandardScaler()\n",
        "X_tr_norm = norm.fit_transform(X_tr)\n",
        "X_te_norm = norm.transform(X_te)\n",
        "\n",
        "print(f\"  Train Split: {X_tr_norm.shape[0]:,} samples\")\n",
        "print(f\"  Test Split: {X_te_norm.shape[0]:,} samples\")\n",
        "print(f\"  Normalization: StandardScaler applied\\n\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# STEP 3: Build Regression Models\n",
        "# ============================================\n",
        "\n",
        "print(\"[STEP 3] MODEL CONSTRUCTION & TRAINING\")\n",
        "print(f\"  Regularization Parameter (Î±): 0.5748\\n\")\n",
        "\n",
        "# Define models\n",
        "regressors = {\n",
        "    'Vanilla Linear': LinearRegression(),\n",
        "    'Ridge (L2)': Ridge(alpha=0.5748, random_state=42),\n",
        "    'Lasso (L1)': Lasso(alpha=0.5748, random_state=42, max_iter=10000)\n",
        "}\n",
        "\n",
        "metrics_collection = {}\n",
        "\n",
        "for model_label, regressor in regressors.items():\n",
        "    print(f\"  {model_label}\")\n",
        "    print(f\"    \" + \"-\"*60)\n",
        "\n",
        "    # Train\n",
        "    regressor.fit(X_tr_norm, y_tr)\n",
        "\n",
        "    # Forecast\n",
        "    y_tr_hat = regressor.predict(X_tr_norm)\n",
        "    y_te_hat = regressor.predict(X_te_norm)\n",
        "\n",
        "    # Compute metrics\n",
        "    r2_train = r2_score(y_tr, y_tr_hat)\n",
        "    r2_test = r2_score(y_te, y_te_hat)\n",
        "    rmse_train = np.sqrt(mean_squared_error(y_tr, y_tr_hat))\n",
        "    rmse_test = np.sqrt(mean_squared_error(y_te, y_te_hat))\n",
        "    overfit = r2_train - r2_test\n",
        "\n",
        "    metrics_collection[model_label] = {\n",
        "        'RÂ² (Train)': r2_train,\n",
        "        'RÂ² (Test)': r2_test,\n",
        "        'RMSE (Train)': rmse_train,\n",
        "        'RMSE (Test)': rmse_test,\n",
        "        'Overfitting': overfit\n",
        "    }\n",
        "\n",
        "    print(f\"    Train: RÂ²={r2_train:.5f} | RMSE=${rmse_train:.2f}K\")\n",
        "    print(f\"    Test:  RÂ²={r2_test:.5f} | RMSE=${rmse_test:.2f}K\")\n",
        "    print(f\"    Generalization Gap: {overfit:.5f}\\n\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# STEP 4: Comparative Analysis\n",
        "# ============================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"[STEP 4] PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "summary_table = pd.DataFrame(metrics_collection).T\n",
        "print(summary_table.round(5).to_string())\n",
        "\n",
        "# Identify champion\n",
        "champion = summary_table['RÂ² (Test)'].idxmax()\n",
        "champion_r2 = summary_table.loc[champion, 'RÂ² (Test)']\n",
        "\n",
        "print(f\"\\n{'ðŸ¥‡ CHAMPION MODEL':^70}\")\n",
        "print(f\"{'-'*70}\")\n",
        "print(f\"Model: {champion}\")\n",
        "print(f\"Test RÂ²: {champion_r2:.5f} ({champion_r2*100:.2f}% variance explained)\")\n",
        "print(f\"Test RMSE: ${summary_table.loc[champion, 'RMSE (Test)']:.2f}K\\n\")\n",
        "\n",
        "# Ranking\n",
        "rank_df = summary_table.sort_values('RÂ² (Test)', ascending=False)\n",
        "print(f\"{'MODEL RANKING':^70}\")\n",
        "for idx, (model_name, metrics) in enumerate(rank_df.iterrows(), 1):\n",
        "    print(f\"  #{idx} | {model_name:<25} | Test RÂ²: {metrics['RÂ² (Test)']:.5f}\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# STEP 5: Export & Summary\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"[STEP 5] RESULTS EXPORT\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "summary_table.to_csv('hitters_regression_comparison.csv')\n",
        "print(\"\\nâœ“ Comparison metrics exported â†’ 'hitters_regression_comparison.csv'\")\n",
        "\n",
        "print(f\"\\nKEY METRICS:\")\n",
        "print(f\"  â€¢ Best Model: {champion}\")\n",
        "print(f\"  â€¢ Variance Explained: {champion_r2*100:.2f}%\")\n",
        "print(f\"  â€¢ Test RMSE: ${summary_table.loc[champion, 'RMSE (Test)']:.2f}K\")\n",
        "print(f\"  â€¢ Generalization Consistency: {(1-summary_table.loc[champion, 'Overfitting'])*100:.1f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz1NhRWs7mep",
        "outputId": "a39ce77e-71fa-45be-e30a-7a8b28d80ab1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "BASEBALL SALARY PREDICTION: LINEAR vs RIDGE vs LASSO\n",
            "======================================================================\n",
            "\n",
            "[STEP 1] DATASET ACQUISITION & CLEANING\n",
            "  Initial Records: 322\n",
            "  Total Null Values: 59\n",
            "  After Null Removal: 263\n",
            "  After Deduplication: 263\n",
            "  Categorical Features Encoded: 3\n",
            "  âœ“ Data cleaning complete\n",
            "\n",
            "[STEP 2] FEATURE ENGINEERING & NORMALIZATION\n",
            "  Input Features (X): 263 samples Ã— 19 variables\n",
            "  Output Variable (y): 263 samples\n",
            "  Train Split: 210 samples\n",
            "  Test Split: 53 samples\n",
            "  Normalization: StandardScaler applied\n",
            "\n",
            "[STEP 3] MODEL CONSTRUCTION & TRAINING\n",
            "  Regularization Parameter (Î±): 0.5748\n",
            "\n",
            "  Vanilla Linear\n",
            "    ------------------------------------------------------------\n",
            "    Train: RÂ²=0.59047 | RMSE=$291.83K\n",
            "    Test:  RÂ²=0.29075 | RMSE=$358.17K\n",
            "    Generalization Gap: 0.29972\n",
            "\n",
            "  Ridge (L2)\n",
            "    ------------------------------------------------------------\n",
            "    Train: RÂ²=0.58816 | RMSE=$292.65K\n",
            "    Test:  RÂ²=0.30004 | RMSE=$355.81K\n",
            "    Generalization Gap: 0.28812\n",
            "\n",
            "  Lasso (L1)\n",
            "    ------------------------------------------------------------\n",
            "    Train: RÂ²=0.58873 | RMSE=$292.45K\n",
            "    Test:  RÂ²=0.29963 | RMSE=$355.92K\n",
            "    Generalization Gap: 0.28911\n",
            "\n",
            "======================================================================\n",
            "[STEP 4] PERFORMANCE COMPARISON\n",
            "======================================================================\n",
            "\n",
            "                RÂ² (Train)  RÂ² (Test)  RMSE (Train)  RMSE (Test)  Overfitting\n",
            "Vanilla Linear     0.59047    0.29075     291.82881    358.16804      0.29972\n",
            "Ridge (L2)         0.58816    0.30004     292.65151    355.81442      0.28812\n",
            "Lasso (L1)         0.58873    0.29963     292.44626    355.91869      0.28911\n",
            "\n",
            "                           ðŸ¥‡ CHAMPION MODEL                           \n",
            "----------------------------------------------------------------------\n",
            "Model: Ridge (L2)\n",
            "Test RÂ²: 0.30004 (30.00% variance explained)\n",
            "Test RMSE: $355.81K\n",
            "\n",
            "                            MODEL RANKING                             \n",
            "  #1 | Ridge (L2)                | Test RÂ²: 0.30004\n",
            "  #2 | Lasso (L1)                | Test RÂ²: 0.29963\n",
            "  #3 | Vanilla Linear            | Test RÂ²: 0.29075\n",
            "\n",
            "======================================================================\n",
            "[STEP 5] RESULTS EXPORT\n",
            "======================================================================\n",
            "\n",
            "âœ“ Comparison metrics exported â†’ 'hitters_regression_comparison.csv'\n",
            "\n",
            "KEY METRICS:\n",
            "  â€¢ Best Model: Ridge (L2)\n",
            "  â€¢ Variance Explained: 30.00%\n",
            "  â€¢ Test RMSE: $355.81K\n",
            "  â€¢ Generalization Consistency: 71.2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(\"CROSS VALIDATION FOR RIDGE AND LASSO REGRESSION\")\n",
        "print(\"Boston House Prediction Dataset\")\n",
        "\n",
        "\n",
        "# STEP 1: Load Boston Housing Dataset\n",
        "print(\"\\nSTEP 1: LOADING BOSTON HOUSING DATASET\")\n",
        "\n",
        "# Load from online CSV (sklearn removed load_boston in v1.2+)\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv')\n",
        "X = df.drop('medv', axis=1).values\n",
        "y = df['medv'].values\n",
        "feature_names = df.drop('medv', axis=1).columns.tolist()\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Features: {list(feature_names)}\")\n",
        "print(f\"Target: PRICE (Median house value in $1000s)\")\n",
        "\n",
        "\n",
        "# STEP 2: Data Preprocessing\n",
        "print(\"\\nSTEP 2: DATA PREPROCESSING\")\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")\n",
        "print(\"Features scaled\")\n",
        "\n",
        "\n",
        "# STEP 3: Ridge Cross Validation (RidgeCV)\n",
        "print(\"\\nSTEP 3: RIDGE CROSS VALIDATION (RidgeCV)\")\n",
        "\n",
        "\n",
        "alphas = np.logspace(-4, 4, 100)  # 100 alpha values\n",
        "ridge_cv = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')\n",
        "ridge_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "print(f\"5-fold cross-validation completed\")\n",
        "print(f\"Best alpha: {ridge_cv.alpha_:.6f}\")\n",
        "\n",
        "\n",
        "# Train with best alpha\n",
        "ridge_best = Ridge(alpha=ridge_cv.alpha_)\n",
        "ridge_best.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "y_test_pred_ridge = ridge_best.predict(X_test_scaled)\n",
        "ridge_test_r2 = r2_score(y_test, y_test_pred_ridge)\n",
        "ridge_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_ridge))\n",
        "\n",
        "\n",
        "print(f\"Test RÂ²: {ridge_test_r2:.6f}\")\n",
        "print(f\"Test RMSE: {ridge_test_rmse:.4f}\")\n",
        "\n",
        "\n",
        "# STEP 4: Lasso Cross Validation (LassoCV)\n",
        "print(\"\\nSTEP 4: LASSO CROSS VALIDATION (LassoCV)\")\n",
        "\n",
        "\n",
        "lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=42, max_iter=10000)\n",
        "lasso_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "print(f\"5-fold cross-validation completed\")\n",
        "print(f\"Best alpha: {lasso_cv.alpha_:.6f}\")\n",
        "\n",
        "\n",
        "# Train with best alpha\n",
        "lasso_best = Lasso(alpha=lasso_cv.alpha_, max_iter=10000)\n",
        "lasso_best.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "y_test_pred_lasso = lasso_best.predict(X_test_scaled)\n",
        "lasso_test_r2 = r2_score(y_test, y_test_pred_lasso)\n",
        "lasso_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_lasso))\n",
        "\n",
        "\n",
        "print(f\"Test RÂ²: {lasso_test_r2:.6f}\")\n",
        "print(f\"Test RMSE: {lasso_test_rmse:.4f}\")\n",
        "\n",
        "\n",
        "# Feature selection\n",
        "non_zero = np.sum(np.abs(lasso_best.coef_) > 1e-10)\n",
        "print(f\"Features selected: {non_zero}/{len(lasso_best.coef_)}\")\n",
        "\n",
        "\n",
        "# STEP 5: Model Comparison\n",
        "print(\"\\nMODEL COMPARISON\")\n",
        "\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    'Ridge (CV)': {\n",
        "        'Optimal Alpha': ridge_cv.alpha_,\n",
        "        'Test RÂ²': ridge_test_r2,\n",
        "        'Test RMSE': ridge_test_rmse\n",
        "    },\n",
        "    'Lasso (CV)': {\n",
        "        'Optimal Alpha': lasso_cv.alpha_,\n",
        "        'Test RÂ²': lasso_test_r2,\n",
        "        'Test RMSE': lasso_test_rmse\n",
        "    }\n",
        "}).T\n",
        "\n",
        "\n",
        "print(results.round(6))\n",
        "\n",
        "\n",
        "best_model = results['Test RÂ²'].idxmax()\n",
        "print(f\"\\nBEST MODEL: {best_model}\")\n",
        "print(f\"Test RÂ²: {results.loc[best_model, 'Test RÂ²']:.6f}\")\n",
        "\n",
        "\n",
        "# Feature importance\n",
        "print(\"\\nFEATURE IMPORTANCE\")\n",
        "\n",
        "\n",
        "print(\"\\nTop 5 Ridge coefficients:\")\n",
        "ridge_imp = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': ridge_best.coef_\n",
        "}).sort_values('Coefficient', key=abs, ascending=False).head(5)\n",
        "print(ridge_imp.to_string(index=False))\n",
        "\n",
        "\n",
        "print(\"\\nTop 5 Lasso coefficients:\")\n",
        "lasso_imp = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': lasso_best.coef_\n",
        "}).sort_values('Coefficient', key=abs, ascending=False).head(5)\n",
        "print(lasso_imp.to_string(index=False))\n",
        "\n",
        "\n",
        "print(\"\\nANALYSIS COMPLETE\")\n"
      ],
      "metadata": {
        "id": "52mteKgZ-jlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "# ================================================\n",
        "# REAL ESTATE VALUATION: REGULARIZED REGRESSION\n",
        "# ================================================\n",
        "\n",
        "print(\"=\"*75)\n",
        "print(\"PREDICTIVE MODELING WITH REGULARIZED LINEAR REGRESSION\")\n",
        "print(\"Dataset: Boston Housing Market Analysis\")\n",
        "print(\"=\"*75)\n",
        "\n",
        "\n",
        "# ================================================\n",
        "# PHASE 1: DATA ACQUISITION\n",
        "# ================================================\n",
        "\n",
        "print(\"\\n[PHASE 1] ACQUIRING & PREPARING DATA\")\n",
        "print(\"-\"*75)\n",
        "\n",
        "# Retrieve housing dataset\n",
        "housing_data = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv')\n",
        "predictors = housing_data.drop('medv', axis=1).values\n",
        "outcomes = housing_data['medv'].values\n",
        "attribute_list = housing_data.drop('medv', axis=1).columns.tolist()\n",
        "\n",
        "print(f\"Dataset Dimensions: {predictors.shape[0]} properties Ã— {predictors.shape[1]} attributes\")\n",
        "print(f\"Target Variable: Median Home Value (thousands USD)\")\n",
        "print(f\"Predictive Features: {', '.join(attribute_list[:3])}...+{len(attribute_list)-3} more\")\n",
        "\n",
        "\n",
        "# ================================================\n",
        "# PHASE 2: DATA SPLITTING & NORMALIZATION\n",
        "# ================================================\n",
        "\n",
        "print(\"\\n[PHASE 2] TRAIN-TEST PARTITIONING & SCALING\")\n",
        "print(\"-\"*75)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    predictors, outcomes, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "normalizer = StandardScaler()\n",
        "X_train_normalized = normalizer.fit_transform(X_train)\n",
        "X_test_normalized = normalizer.transform(X_test)\n",
        "\n",
        "print(f\"Training Subset: {X_train_normalized.shape[0]} samples\")\n",
        "print(f\"Testing Subset: {X_test_normalized.shape[0]} samples\")\n",
        "print(f\"Scaling Applied: StandardScaler (mean=0, std=1)\")\n",
        "\n",
        "\n",
        "# ================================================\n",
        "# PHASE 3: RIDGE REGRESSION WITH CV\n",
        "# ================================================\n",
        "\n",
        "print(\"\\n[PHASE 3] RIDGE REGRESSION - HYPERPARAMETER OPTIMIZATION\")\n",
        "print(\"-\"*75)\n",
        "\n",
        "alpha_candidates = np.logspace(-4, 4, 100)\n",
        "ridge_optimizer = RidgeCV(alphas=alpha_candidates, cv=5, scoring='neg_mean_squared_error')\n",
        "ridge_optimizer.fit(X_train_normalized, y_train)\n",
        "\n",
        "optimal_ridge_alpha = ridge_optimizer.alpha_\n",
        "\n",
        "ridge_model = Ridge(alpha=optimal_ridge_alpha)\n",
        "ridge_model.fit(X_train_normalized, y_train)\n",
        "\n",
        "ridge_predictions = ridge_model.predict(X_test_normalized)\n",
        "ridge_r2 = r2_score(y_test, ridge_predictions)\n",
        "ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_predictions))\n",
        "\n",
        "print(f\"Validation Strategy: 5-Fold Cross-Validation\")\n",
        "print(f\"Alpha Search Space: [{alpha_candidates[0]:.2e}, {alpha_candidates[-1]:.2e}]\")\n",
        "print(f\"Optimal Alpha: {optimal_ridge_alpha:.6f}\")\n",
        "print(f\"â”œâ”€ Test RÂ² Score: {ridge_r2:.6f}\")\n",
        "print(f\"â””â”€ Test RMSE: ${ridge_rmse:.4f}K\")\n",
        "\n",
        "\n",
        "# ================================================\n",
        "# PHASE 4: LASSO REGRESSION WITH CV\n",
        "# ================================================\n",
        "\n",
        "print(\"\\n[PHASE 4] LASSO REGRESSION - HYPERPARAMETER OPTIMIZATION\")\n",
        "print(\"-\"*75)\n",
        "\n",
        "lasso_optimizer = LassoCV(alphas=alpha_candidates, cv=5, random_state=42, max_iter=10000)\n",
        "lasso_optimizer.fit(X_train_normalized, y_train)\n",
        "\n",
        "optimal_lasso_alpha = lasso_optimizer.alpha_\n",
        "\n",
        "lasso_model = Lasso(alpha=optimal_lasso_alpha, max_iter=10000)\n",
        "lasso_model.fit(X_train_normalized, y_train)\n",
        "\n",
        "lasso_predictions = lasso_model.predict(X_test_normalized)\n",
        "lasso_r2 = r2_score(y_test, lasso_predictions)\n",
        "lasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_predictions))\n",
        "\n",
        "active_features = np.sum(np.abs(lasso_model.coef_) > 1e-10)\n",
        "\n",
        "print(f\"Validation Strategy: 5-Fold Cross-Validation\")\n",
        "print(f\"Alpha Search Space: [{alpha_candidates[0]:.2e}, {alpha_candidates[-1]:.2e}]\")\n",
        "print(f\"Optimal Alpha: {optimal_lasso_alpha:.6f}\")\n",
        "print(f\"â”œâ”€ Test RÂ² Score: {lasso_r2:.6f}\")\n",
        "print(f\"â”œâ”€ Test RMSE: ${lasso_rmse:.4f}K\")\n",
        "print(f\"â””â”€ Active Features: {active_features} out of {len(lasso_model.coef_)}\")\n",
        "\n",
        "\n",
        "# ================================================\n",
        "# PHASE 5: COMPARATIVE EVALUATION\n",
        "# ================================================\n",
        "\n",
        "print(\"\\n[PHASE 5] PERFORMANCE METRICS & COMPARISON\")\n",
        "print(\"-\"*75)\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Ridge (L2)': {\n",
        "        'Optimal Î±': optimal_ridge_alpha,\n",
        "        'Test RÂ²': ridge_r2,\n",
        "        'Test RMSE': ridge_rmse,\n",
        "        'Features': len(ridge_model.coef_)\n",
        "    },\n",
        "    'Lasso (L1)': {\n",
        "        'Optimal Î±': optimal_lasso_alpha,\n",
        "        'Test RÂ²': lasso_r2,\n",
        "        'Test RMSE': lasso_rmse,\n",
        "        'Features': active_features\n",
        "    }\n",
        "}).T\n",
        "\n",
        "print(comparison.round(6).to_string())\n",
        "\n",
        "winner = comparison['Test RÂ²'].idxmax()\n",
        "best_score = comparison.loc[winner, 'Test RÂ²']\n",
        "\n",
        "print(f\"\\n{'â­ SUPERIOR MODEL':^75}\")\n",
        "print(f\"{winner} | RÂ² = {best_score:.6f}\")\n",
        "\n",
        "\n",
        "# ================================================\n",
        "# PHASE 6: COEFFICIENT ANALYSIS\n",
        "# ================================================\n",
        "\n",
        "print(\"\\n[PHASE 6] FEATURE CONTRIBUTION ANALYSIS\")\n",
        "print(\"-\"*75)\n",
        "\n",
        "print(\"\\nRIDGE - Top 5 Influential Features:\")\n",
        "ridge_features = pd.DataFrame({\n",
        "    'Feature': attribute_list,\n",
        "    'Weight': ridge_model.coef_\n",
        "}).reindex(pd.DataFrame({\n",
        "    'Feature': attribute_list,\n",
        "    'Weight': ridge_model.coef_\n",
        "}).sort_values('Weight', key=abs, ascending=False).index).head(5)\n",
        "\n",
        "for i, (_, row) in enumerate(ridge_features.iterrows(), 1):\n",
        "    print(f\"  {i}. {row['Feature']:<15} â†’ {row['Weight']:>10.6f}\")\n",
        "\n",
        "print(\"\\nLASSO - Top 5 Influential Features:\")\n",
        "lasso_features = pd.DataFrame({\n",
        "    'Feature': attribute_list,\n",
        "    'Weight': lasso_model.coef_\n",
        "}).reindex(pd.DataFrame({\n",
        "    'Feature': attribute_list,\n",
        "    'Weight': lasso_model.coef_\n",
        "}).sort_values('Weight', key=abs, ascending=False).index).head(5)\n",
        "\n",
        "for i, (_, row) in enumerate(lasso_features.iterrows(), 1):\n",
        "    print(f\"  {i}. {row['Feature']:<15} â†’ {row['Weight']:>10.6f}\")\n",
        "\n",
        "\n",
        "# ================================================\n",
        "# SUMMARY\n",
        "# ================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*75)\n",
        "print(\"ANALYSIS SUMMARY\")\n",
        "print(\"=\"*75)\n",
        "print(f\"Ridge Î±: {optimal_ridge_alpha:.6f} â†’ RÂ²: {ridge_r2:.4f} | RMSE: ${ridge_rmse:.2f}K\")\n",
        "print(f\"Lasso Î±: {optimal_lasso_alpha:.6f} â†’ RÂ²: {lasso_r2:.4f} | RMSE: ${lasso_rmse:.2f}K\")\n",
        "print(\"\\nâœ“ Cross-validation complete | Models ready for deployment\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv5YBKNDAHDa",
        "outputId": "bc1466bd-dfa4-42a3-9fc7-13fb13bdf69a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========================================================================\n",
            "PREDICTIVE MODELING WITH REGULARIZED LINEAR REGRESSION\n",
            "Dataset: Boston Housing Market Analysis\n",
            "===========================================================================\n",
            "\n",
            "[PHASE 1] ACQUIRING & PREPARING DATA\n",
            "---------------------------------------------------------------------------\n",
            "Dataset Dimensions: 506 properties Ã— 13 attributes\n",
            "Target Variable: Median Home Value (thousands USD)\n",
            "Predictive Features: crim, zn, indus...+10 more\n",
            "\n",
            "[PHASE 2] TRAIN-TEST PARTITIONING & SCALING\n",
            "---------------------------------------------------------------------------\n",
            "Training Subset: 404 samples\n",
            "Testing Subset: 102 samples\n",
            "Scaling Applied: StandardScaler (mean=0, std=1)\n",
            "\n",
            "[PHASE 3] RIDGE REGRESSION - HYPERPARAMETER OPTIMIZATION\n",
            "---------------------------------------------------------------------------\n",
            "Validation Strategy: 5-Fold Cross-Validation\n",
            "Alpha Search Space: [1.00e-04, 1.00e+04]\n",
            "Optimal Alpha: 2.310130\n",
            "â”œâ”€ Test RÂ² Score: 0.668074\n",
            "â””â”€ Test RMSE: $4.9337K\n",
            "\n",
            "[PHASE 4] LASSO REGRESSION - HYPERPARAMETER OPTIMIZATION\n",
            "---------------------------------------------------------------------------\n",
            "Validation Strategy: 5-Fold Cross-Validation\n",
            "Alpha Search Space: [1.00e-04, 1.00e+04]\n",
            "Optimal Alpha: 0.000100\n",
            "â”œâ”€ Test RÂ² Score: 0.668755\n",
            "â”œâ”€ Test RMSE: $4.9286K\n",
            "â””â”€ Active Features: 13 out of 13\n",
            "\n",
            "[PHASE 5] PERFORMANCE METRICS & COMPARISON\n",
            "---------------------------------------------------------------------------\n",
            "            Optimal Î±   Test RÂ²  Test RMSE  Features\n",
            "Ridge (L2)    2.31013  0.668074   4.933697      13.0\n",
            "Lasso (L1)    0.00010  0.668755   4.928636      13.0\n",
            "\n",
            "                             â­ SUPERIOR MODEL                              \n",
            "Lasso (L1) | RÂ² = 0.668755\n",
            "\n",
            "[PHASE 6] FEATURE CONTRIBUTION ANALYSIS\n",
            "---------------------------------------------------------------------------\n",
            "\n",
            "RIDGE - Top 5 Influential Features:\n",
            "  1. lstat           â†’  -3.582821\n",
            "  2. rm              â†’   3.158988\n",
            "  3. dis             â†’  -2.998344\n",
            "  4. rad             â†’   2.078872\n",
            "  5. ptratio         â†’  -2.015331\n",
            "\n",
            "LASSO - Top 5 Influential Features:\n",
            "  1. lstat           â†’  -3.611634\n",
            "  2. rm              â†’   3.145303\n",
            "  3. dis             â†’  -3.081205\n",
            "  4. rad             â†’   2.249600\n",
            "  5. ptratio         â†’  -2.037566\n",
            "\n",
            "===========================================================================\n",
            "ANALYSIS SUMMARY\n",
            "===========================================================================\n",
            "Ridge Î±: 2.310130 â†’ RÂ²: 0.6681 | RMSE: $4.93K\n",
            "Lasso Î±: 0.000100 â†’ RÂ²: 0.6688 | RMSE: $4.93K\n",
            "\n",
            "âœ“ Cross-validation complete | Models ready for deployment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# FLORAL CLASSIFICATION SYSTEM - IRIS SPECIES RECOGNITION\n",
        "# ================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MULTICLASS PATTERN RECOGNITION: IRIS FLOWER CLASSIFICATION\")\n",
        "print(\"Methodology: Logistic Regression with One-vs-Rest Strategy\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# SEGMENT A: DATASET INITIALIZATION\n",
        "# ================================================================\n",
        "\n",
        "print(\"\\n[SEGMENT A] BOTANICAL DATASET ACQUISITION\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Import iris flower measurements\n",
        "iris_collection = load_iris()\n",
        "measurements = iris_collection.data\n",
        "classifications = iris_collection.target\n",
        "measurement_names = iris_collection.feature_names\n",
        "species_names = iris_collection.target_names\n",
        "\n",
        "print(f\"Total Specimens: {measurements.shape[0]}\")\n",
        "print(f\"Attributes Per Specimen: {measurements.shape[1]}\")\n",
        "print(f\"Species Categories: {len(species_names)}\")\n",
        "print(f\"\\nFloral Attributes:\")\n",
        "for idx, attr in enumerate(measurement_names, 1):\n",
        "    print(f\"  {idx}. {attr}\")\n",
        "print(f\"\\nTarget Species:\")\n",
        "for idx, species in enumerate(species_names):\n",
        "    print(f\"  {idx} â†’ {species}\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# SEGMENT B: DATASET PARTITIONING & NORMALIZATION\n",
        "# ================================================================\n",
        "\n",
        "print(\"\\n[SEGMENT B] TRAIN-TEST STRATIFICATION & FEATURE SCALING\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Stratified split to preserve class distribution\n",
        "train_measurements, test_measurements, train_labels, test_labels = train_test_split(\n",
        "    measurements, classifications, test_size=0.2, random_state=42, stratify=classifications\n",
        ")\n",
        "\n",
        "print(f\"Training Cohort: {train_measurements.shape[0]} specimens\")\n",
        "print(f\"Testing Cohort: {test_measurements.shape[0]} specimens\")\n",
        "\n",
        "# Standardization pipeline\n",
        "scaler = StandardScaler()\n",
        "train_scaled = scaler.fit_transform(train_measurements)\n",
        "test_scaled = scaler.transform(test_measurements)\n",
        "\n",
        "print(f\"Normalization Method: StandardScaler (Î¼=0, Ïƒ=1)\")\n",
        "print(f\"âœ“ Data preparation complete\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# SEGMENT C: MULTICLASS LOGISTIC REGRESSION (ONE-VS-REST)\n",
        "# ================================================================\n",
        "\n",
        "print(\"\\n[SEGMENT C] CLASSIFIER TRAINING - ONE-VS-REST APPROACH\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Initialize OvR classifier\n",
        "classifier = LogisticRegression(\n",
        "    multi_class='ovr',\n",
        "    solver='lbfgs',\n",
        "    max_iter=200,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training configuration:\")\n",
        "print(f\"  â€¢ Strategy: One-vs-Rest (OvR)\")\n",
        "print(f\"  â€¢ Binary Classifiers: {len(species_names)}\")\n",
        "print(f\"  â€¢ Optimization: LBFGS\")\n",
        "print(f\"  â€¢ Max Iterations: 200\")\n",
        "print(f\"\\nFitting model on training data...\")\n",
        "\n",
        "classifier.fit(train_scaled, train_labels)\n",
        "\n",
        "print(\"âœ“ Model convergence achieved\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# SEGMENT D: PREDICTIVE PERFORMANCE ANALYSIS\n",
        "# ================================================================\n",
        "\n",
        "print(\"\\n[SEGMENT D] CLASSIFICATION ACCURACY & DIAGNOSTICS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Generate predictions\n",
        "train_predictions = classifier.predict(train_scaled)\n",
        "test_predictions = classifier.predict(test_scaled)\n",
        "confidence_scores = classifier.predict_proba(test_scaled)\n",
        "\n",
        "# Compute metrics\n",
        "train_acc = accuracy_score(train_labels, train_predictions)\n",
        "test_acc = accuracy_score(test_labels, test_predictions)\n",
        "\n",
        "print(f\"\\nAccuracy Metrics:\")\n",
        "print(f\"  Training Set:  {train_acc*100:>6.2f}%\")\n",
        "print(f\"  Testing Set:   {test_acc*100:>6.2f}%\")\n",
        "print(f\"  Generalization Gap: {(train_acc - test_acc)*100:>5.2f}%\")\n",
        "\n",
        "print(f\"\\nDetailed Classification Breakdown:\")\n",
        "print(classification_report(test_labels, test_predictions, target_names=species_names))\n",
        "\n",
        "print(\"\\nPrediction Error Matrix:\")\n",
        "error_matrix = confusion_matrix(test_labels, test_predictions)\n",
        "error_df = pd.DataFrame(error_matrix, index=species_names, columns=species_names)\n",
        "print(error_df.to_string())\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# SEGMENT E: BINARY CLASSIFIER INSPECTION\n",
        "# ================================================================\n",
        "\n",
        "print(\"\\n[SEGMENT E] ONE-VS-REST BINARY DECISION BOUNDARIES\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "model_weights = classifier.coef_\n",
        "model_biases = classifier.intercept_\n",
        "\n",
        "for class_idx, species in enumerate(species_names):\n",
        "    print(f\"\\nðŸ“Š Classifier #{class_idx + 1}: {species.upper()} vs Others\")\n",
        "    print(f\"  Bias Term: {model_biases[class_idx]:>8.4f}\")\n",
        "    print(f\"  Feature Weights:\")\n",
        "\n",
        "    for feat_idx, feat_name in enumerate(measurement_names):\n",
        "        weight_val = model_weights[class_idx][feat_idx]\n",
        "        direction = \"â†‘\" if weight_val > 0 else \"â†“\"\n",
        "        print(f\"    {direction} {feat_name:<25} {weight_val:>10.4f}\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# SEGMENT F: FEATURE CONTRIBUTION RANKING\n",
        "# ================================================================\n",
        "\n",
        "print(\"\\n[SEGMENT F] GLOBAL FEATURE IMPORTANCE SCORES\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Calculate mean absolute weights across all classifiers\n",
        "importance_scores = np.abs(model_weights).mean(axis=0)\n",
        "importance_ranking = pd.DataFrame({\n",
        "    'Measurement': measurement_names,\n",
        "    'Impact_Score': importance_scores\n",
        "}).sort_values('Impact_Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nRanking (Most to Least Discriminative):\")\n",
        "for rank, (_, row) in enumerate(importance_ranking.iterrows(), 1):\n",
        "    bar_length = int(row['Impact_Score'] * 50)\n",
        "    bar = \"â–ˆ\" * bar_length\n",
        "    print(f\"  {rank}. {row['Measurement']:<25} {bar} {row['Impact_Score']:.4f}\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# SEGMENT G: INDIVIDUAL SPECIMEN ANALYSIS\n",
        "# ================================================================\n",
        "\n",
        "print(\"\\n[SEGMENT G] SAMPLE CLASSIFICATION RESULTS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "display_count = min(5, len(test_measurements))\n",
        "for sample_num in range(display_count):\n",
        "    actual_species = species_names[test_labels[sample_num]]\n",
        "    predicted_species = species_names[test_predictions[sample_num]]\n",
        "    probabilities = confidence_scores[sample_num]\n",
        "    match_status = \"âœ“ MATCH\" if test_labels[sample_num] == test_predictions[sample_num] else \"âœ— MISMATCH\"\n",
        "\n",
        "    print(f\"\\nSpecimen #{sample_num + 1}:\")\n",
        "    print(f\"  Ground Truth: {actual_species}\")\n",
        "    print(f\"  Model Output: {predicted_species}\")\n",
        "    print(f\"  Status: {match_status}\")\n",
        "    print(f\"  Confidence Distribution:\")\n",
        "\n",
        "    for class_idx, species in enumerate(species_names):\n",
        "        conf_pct = probabilities[class_idx] * 100\n",
        "        bar_len = int(conf_pct / 5)\n",
        "        bar = \"â–¬\" * bar_len\n",
        "        print(f\"    â€¢ {species:<15} {bar} {conf_pct:>6.2f}%\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# RESULTS SUMMARY\n",
        "# ================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLASSIFICATION PIPELINE COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Model Performance Summary:\")\n",
        "print(f\"   Dataset: Iris Flower Database (150 specimens)\")\n",
        "print(f\"   Attributes: 4 morphological measurements\")\n",
        "print(f\"   Target Classes: 3 species categories\")\n",
        "print(f\"   Classification Method: Logistic Regression (OvR)\")\n",
        "print(f\"   Test Accuracy: {test_acc*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nâ­ Top Predictive Features:\")\n",
        "for rank, (_, row) in enumerate(importance_ranking.head(2).iterrows(), 1):\n",
        "    print(f\"   {rank}. {row['Measurement']}\")\n",
        "\n",
        "print(f\"\\nâœ“ All classifiers trained and evaluated successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY0GaD0ZAKta",
        "outputId": "3d15575e-0325-4a86-cbb8-46f482832a14"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "MULTICLASS PATTERN RECOGNITION: IRIS FLOWER CLASSIFICATION\n",
            "Methodology: Logistic Regression with One-vs-Rest Strategy\n",
            "================================================================================\n",
            "\n",
            "[SEGMENT A] BOTANICAL DATASET ACQUISITION\n",
            "--------------------------------------------------------------------------------\n",
            "Total Specimens: 150\n",
            "Attributes Per Specimen: 4\n",
            "Species Categories: 3\n",
            "\n",
            "Floral Attributes:\n",
            "  1. sepal length (cm)\n",
            "  2. sepal width (cm)\n",
            "  3. petal length (cm)\n",
            "  4. petal width (cm)\n",
            "\n",
            "Target Species:\n",
            "  0 â†’ setosa\n",
            "  1 â†’ versicolor\n",
            "  2 â†’ virginica\n",
            "\n",
            "[SEGMENT B] TRAIN-TEST STRATIFICATION & FEATURE SCALING\n",
            "--------------------------------------------------------------------------------\n",
            "Training Cohort: 120 specimens\n",
            "Testing Cohort: 30 specimens\n",
            "Normalization Method: StandardScaler (Î¼=0, Ïƒ=1)\n",
            "âœ“ Data preparation complete\n",
            "\n",
            "[SEGMENT C] CLASSIFIER TRAINING - ONE-VS-REST APPROACH\n",
            "--------------------------------------------------------------------------------\n",
            "Training configuration:\n",
            "  â€¢ Strategy: One-vs-Rest (OvR)\n",
            "  â€¢ Binary Classifiers: 3\n",
            "  â€¢ Optimization: LBFGS\n",
            "  â€¢ Max Iterations: 200\n",
            "\n",
            "Fitting model on training data...\n",
            "âœ“ Model convergence achieved\n",
            "\n",
            "[SEGMENT D] CLASSIFICATION ACCURACY & DIAGNOSTICS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Accuracy Metrics:\n",
            "  Training Set:   95.00%\n",
            "  Testing Set:    90.00%\n",
            "  Generalization Gap:  5.00%\n",
            "\n",
            "Detailed Classification Breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       0.89      0.80      0.84        10\n",
            "   virginica       0.82      0.90      0.86        10\n",
            "\n",
            "    accuracy                           0.90        30\n",
            "   macro avg       0.90      0.90      0.90        30\n",
            "weighted avg       0.90      0.90      0.90        30\n",
            "\n",
            "\n",
            "Prediction Error Matrix:\n",
            "            setosa  versicolor  virginica\n",
            "setosa          10           0          0\n",
            "versicolor       0           8          2\n",
            "virginica        0           1          9\n",
            "\n",
            "[SEGMENT E] ONE-VS-REST BINARY DECISION BOUNDARIES\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ðŸ“Š Classifier #1: SETOSA vs Others\n",
            "  Bias Term:  -2.4137\n",
            "  Feature Weights:\n",
            "    â†“ sepal length (cm)            -1.0768\n",
            "    â†‘ sepal width (cm)              1.1208\n",
            "    â†“ petal length (cm)            -1.6920\n",
            "    â†“ petal width (cm)             -1.5553\n",
            "\n",
            "ðŸ“Š Classifier #2: VERSICOLOR vs Others\n",
            "  Bias Term:  -0.9466\n",
            "  Feature Weights:\n",
            "    â†‘ sepal length (cm)             0.0630\n",
            "    â†“ sepal width (cm)             -1.2661\n",
            "    â†‘ petal length (cm)             0.8575\n",
            "    â†“ petal width (cm)             -0.9067\n",
            "\n",
            "ðŸ“Š Classifier #3: VIRGINICA vs Others\n",
            "  Bias Term:  -3.4273\n",
            "  Feature Weights:\n",
            "    â†‘ sepal length (cm)             0.2365\n",
            "    â†“ sepal width (cm)             -0.3939\n",
            "    â†‘ petal length (cm)             2.1521\n",
            "    â†‘ petal width (cm)              2.9450\n",
            "\n",
            "[SEGMENT F] GLOBAL FEATURE IMPORTANCE SCORES\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Ranking (Most to Least Discriminative):\n",
            "  1. petal width (cm)          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1.8023\n",
            "  2. petal length (cm)         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1.5672\n",
            "  3. sepal width (cm)          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.9269\n",
            "  4. sepal length (cm)         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.4588\n",
            "\n",
            "[SEGMENT G] SAMPLE CLASSIFICATION RESULTS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Specimen #1:\n",
            "  Ground Truth: setosa\n",
            "  Model Output: setosa\n",
            "  Status: âœ“ MATCH\n",
            "  Confidence Distribution:\n",
            "    â€¢ setosa          â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬  77.43%\n",
            "    â€¢ versicolor      â–¬â–¬â–¬â–¬  22.57%\n",
            "    â€¢ virginica          0.00%\n",
            "\n",
            "Specimen #2:\n",
            "  Ground Truth: virginica\n",
            "  Model Output: virginica\n",
            "  Status: âœ“ MATCH\n",
            "  Confidence Distribution:\n",
            "    â€¢ setosa             0.64%\n",
            "    â€¢ versicolor      â–¬â–¬â–¬â–¬â–¬â–¬  31.78%\n",
            "    â€¢ virginica       â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬  67.58%\n",
            "\n",
            "Specimen #3:\n",
            "  Ground Truth: versicolor\n",
            "  Model Output: versicolor\n",
            "  Status: âœ“ MATCH\n",
            "  Confidence Distribution:\n",
            "    â€¢ setosa          â–¬â–¬  14.85%\n",
            "    â€¢ versicolor      â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬  83.81%\n",
            "    â€¢ virginica          1.33%\n",
            "\n",
            "Specimen #4:\n",
            "  Ground Truth: versicolor\n",
            "  Model Output: versicolor\n",
            "  Status: âœ“ MATCH\n",
            "  Confidence Distribution:\n",
            "    â€¢ setosa          â–¬â–¬  10.29%\n",
            "    â€¢ versicolor      â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬  88.26%\n",
            "    â€¢ virginica          1.45%\n",
            "\n",
            "Specimen #5:\n",
            "  Ground Truth: setosa\n",
            "  Model Output: setosa\n",
            "  Status: âœ“ MATCH\n",
            "  Confidence Distribution:\n",
            "    â€¢ setosa          â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬  84.25%\n",
            "    â€¢ versicolor      â–¬â–¬â–¬  15.75%\n",
            "    â€¢ virginica          0.00%\n",
            "\n",
            "================================================================================\n",
            "CLASSIFICATION PIPELINE COMPLETE\n",
            "================================================================================\n",
            "\n",
            "ðŸ“ˆ Model Performance Summary:\n",
            "   Dataset: Iris Flower Database (150 specimens)\n",
            "   Attributes: 4 morphological measurements\n",
            "   Target Classes: 3 species categories\n",
            "   Classification Method: Logistic Regression (OvR)\n",
            "   Test Accuracy: 90.00%\n",
            "\n",
            "â­ Top Predictive Features:\n",
            "   1. petal width (cm)\n",
            "   2. petal length (cm)\n",
            "\n",
            "âœ“ All classifiers trained and evaluated successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}